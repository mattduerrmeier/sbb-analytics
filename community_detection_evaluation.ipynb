{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9eaa0db-2ece-4722-8380-510700bc6d9b",
   "metadata": {},
   "source": [
    "# 1. Imports and Loading the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:49:14.116847Z",
     "start_time": "2024-05-21T16:49:12.092623Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bf95ed-350b-4058-8e10-321af0450c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:49:14.225596Z",
     "start_time": "2024-05-21T16:49:14.123043Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"data/sbb.edgelist\", delimiter=\";\", create_using=nx.Graph)\n",
    "\n",
    "connected_comp = nx.connected_components(G)\n",
    "max_connected_comp = max(connected_comp)\n",
    "G = G.subgraph(max_connected_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e58d9-229c-4798-883d-1c73c1603233",
   "metadata": {},
   "source": [
    "# 2. Benchmark: Iteration Time, Silhouette Index and Modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5435d66-d404-40e9-9575-6f28fd89b9ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:50:19.812423Z",
     "start_time": "2024-05-21T16:49:14.229665Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import time\n",
    "import os\n",
    "\n",
    "# create directory to save benchmarks results\n",
    "if not os.path.exists(\"data/benchmarks\"):\n",
    "    os.makedirs(\"data/benchmarks\")\n",
    "\n",
    "\n",
    "def multi_iter_benchmark(G, community_detec_func, iters=10, time_threshold=12, leiden=False):\n",
    "    times = []\n",
    "    scores_hops = []\n",
    "    scores_coords = []\n",
    "    modularities = []\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        partitions = community_detec_func(G)\n",
    "        t, s_hops, s_coords, m = single_iter_benchmark(G, partitions, time_threshold, leiden)\n",
    "        times.append(t)\n",
    "        scores_hops.append(s_hops)\n",
    "        scores_coords.append(s_coords)\n",
    "        modularities.append(m)\n",
    "\n",
    "    max_len_iteration = max([len(t) for t in times])\n",
    "    \n",
    "    for (t, s_hops, s_coords, m) in zip(times, scores_hops, scores_coords, modularities):\n",
    "        while len(t) < max_len_iteration:\n",
    "            t.append(np.nan)\n",
    "            s_hops.append(np.nan)\n",
    "            s_coords.append(np.nan)\n",
    "            m.append(np.nan)\n",
    "\n",
    "    mean_times = np.nanmean(times, axis=0)\n",
    "    mean_scores_hops = np.nanmean(scores_hops, axis=0)\n",
    "    mean_scores_coords = np.nanmean(scores_coords, axis=0)\n",
    "    mean_modularities = np.nanmean(modularities, axis=0)\n",
    "\n",
    "    return mean_times, mean_scores_hops, mean_scores_coords, mean_modularities\n",
    "\n",
    "# iteration benchmarking function\n",
    "# you must give it a partition scheme before entering the function\n",
    "# time_threshold is the maximum number of hours the benchmark should run for. Useful for Girvan-Newman\n",
    "def single_iter_benchmark(G, partitions, time_threshold=12, leiden=False):\n",
    "    timeout = time.time() + 60 * 60 * time_threshold\n",
    "\n",
    "    i_time = []\n",
    "    i_silhouette_hops = []\n",
    "    i_silhouette_coords = []\n",
    "    i_modularity = []\n",
    "    \n",
    "    while True:\n",
    "        # Timeout necessary for Girvan Newman\n",
    "        if time.time() > timeout:\n",
    "            break\n",
    "            \n",
    "        start = time.time()\n",
    "        communities = next(partitions, 'finished')\n",
    "        stop = time.time()\n",
    "        \n",
    "        if communities == 'finished':\n",
    "            break\n",
    "        \n",
    "        time_diff = stop - start\n",
    "        i_time.append(time_diff)\n",
    "\n",
    "        # Necessary for leiden: convert node ids to node names\n",
    "        if leiden == True:\n",
    "            communities = convert_leiden(G, communities)\n",
    "        \n",
    "        i_silhouette_hops.append(silhouette_hops(G, communities))\n",
    "        i_silhouette_coords.append(silhouette_coordinates(G, communities))\n",
    "        i_modularity.append(nx.community.modularity(G, communities))\n",
    "        \n",
    "        final_communities = communities\n",
    "        \n",
    "    return i_time, i_silhouette_hops, i_silhouette_coords, i_modularity\n",
    "\n",
    "\n",
    "# silhouette score function\n",
    "def silhouette_hops(G, found_communities):\n",
    "    # make a dictionary with node as key and final community as value\n",
    "    node_to_community = {node: comm_idx for comm_idx, community in enumerate(found_communities) for node in community}\n",
    "    # list all final communities by indexing with keys\n",
    "    comm_labels = [node_to_community[node] for node in sorted(G.nodes())]\n",
    "    \n",
    "    # calc silhouette score by hop matrix as an array and the communities as labels\n",
    "    sil_score = silhouette_score(hop_mat, comm_labels, metric='precomputed', random_state=42)\n",
    "\n",
    "    return sil_score\n",
    "\n",
    "# silhouette score function\n",
    "def silhouette_coordinates(G, found_communities):\n",
    "    # make a dictionary with node as key and final community as value\n",
    "    node_to_community = {node: comm_idx for comm_idx, community in enumerate(found_communities) for node in community}\n",
    "    # list all final communities by indexing with keys\n",
    "    comm_labels = [node_to_community[node] for node in sorted(G.nodes())]\n",
    "    \n",
    "    # calc silhouette score by inputting coordinate matrix as an array and the communities as labels\n",
    "    sil_score = silhouette_score(coordinates_mat, comm_labels, metric='euclidean', random_state=42)\n",
    "\n",
    "    return sil_score\n",
    "\n",
    "# create the coordinate matrix\n",
    "stations_gdf = gpd.read_file('data/stations.geojson')\n",
    "stations_gdf = stations_gdf[stations_gdf[\"station_name\"].isin(list(G.nodes))] # make sure that G.nodes is the Largest Connected Component Graph\n",
    "coordinates_mat = np.column_stack((stations_gdf[\"geometry\"].x, stations_gdf[\"geometry\"].y))\n",
    "\n",
    "# create the hop matrix\n",
    "length = dict(nx.all_pairs_shortest_path_length(G))\n",
    "df = pd.DataFrame(length)\n",
    "df = df.sort_index().sort_index(axis=1)\n",
    "hop_mat = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477310b-8227-42fc-a25d-4befaaed9279",
   "metadata": {},
   "source": [
    "## 2.1 Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1b9ca0-f9e3-48da-a110-a8e7fff6ac27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:50:49.670600Z",
     "start_time": "2024-05-21T16:50:19.817507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       time  silhouette hops  silhouette coordinates  modularity\n0  0.327464         0.344530                0.140315    0.561615\n1  0.085966         0.310990                0.046505    0.817479\n2  0.028904         0.220866               -0.070725    0.895794\n3  0.010234         0.147164               -0.046892    0.906167\n4  0.004340         0.144790               -0.034408    0.906612",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>silhouette hops</th>\n      <th>silhouette coordinates</th>\n      <th>modularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.327464</td>\n      <td>0.344530</td>\n      <td>0.140315</td>\n      <td>0.561615</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.085966</td>\n      <td>0.310990</td>\n      <td>0.046505</td>\n      <td>0.817479</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.028904</td>\n      <td>0.220866</td>\n      <td>-0.070725</td>\n      <td>0.895794</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.010234</td>\n      <td>0.147164</td>\n      <td>-0.046892</td>\n      <td>0.906167</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.004340</td>\n      <td>0.144790</td>\n      <td>-0.034408</td>\n      <td>0.906612</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, s_hops, s_coords, m = multi_iter_benchmark(G, nx.community.louvain_partitions)\n",
    "\n",
    "louvain_bench = pd.DataFrame(zip(t, s_hops, s_coords, m), columns=[\"time\", \"silhouette hops\", \"silhouette coordinates\", \"modularity\"])\n",
    "louvain_bench.to_csv(\"./data/benchmarks/louvain.csv\", index=False)\n",
    "louvain_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d39a54a53c6f4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Louvain Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32b71913b8bab815",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T16:58:21.922601Z",
     "start_time": "2024-05-21T16:57:07.374387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       time  silhouette hops  silhouette coordinates  modularity\n0  4.637026         0.344104                0.139619    0.556008\n1  0.513469         0.278283                0.032737    0.777300\n2  0.201005         0.215072               -0.096429    0.872622\n3  0.067714         0.125470               -0.140040    0.900634",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>silhouette hops</th>\n      <th>silhouette coordinates</th>\n      <th>modularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.637026</td>\n      <td>0.344104</td>\n      <td>0.139619</td>\n      <td>0.556008</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.513469</td>\n      <td>0.278283</td>\n      <td>0.032737</td>\n      <td>0.777300</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.201005</td>\n      <td>0.215072</td>\n      <td>-0.096429</td>\n      <td>0.872622</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.067714</td>\n      <td>0.125470</td>\n      <td>-0.140040</td>\n      <td>0.900634</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from louvain import louvain_generator\n",
    "# remade the graph due to a frozen graph error\n",
    "G = nx.read_edgelist(\"data/sbb.edgelist\", delimiter=\";\", create_using=nx.Graph)\n",
    "connected_comp = nx.connected_components(G)\n",
    "max_connected_comp = max(connected_comp)\n",
    "sub_G = G.subgraph(max_connected_comp)\n",
    "G = nx.Graph(sub_G)\n",
    "\n",
    "t, s_hops, s_coords, m = multi_iter_benchmark(G, louvain_generator)\n",
    "\n",
    "louvain_imp_bench = pd.DataFrame(zip(t, s_hops, s_coords, m), columns=[\"time\", \"silhouette hops\", \"silhouette coordinates\", \"modularity\"])\n",
    "louvain_imp_bench.to_csv(\"./data/benchmarks/louvain_imp.csv\", index=False)\n",
    "louvain_imp_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e9247-910f-4c22-84a0-2b92ba516fa7",
   "metadata": {},
   "source": [
    "## 2.3 Girvan-Newman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70463f5-6dc9-4d3e-9151-fa1168b9a1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:59:10.845779Z",
     "start_time": "2024-05-21T17:06:28.939980Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "3 columns passed, passed data had 4 columns",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    938\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_or_indexify_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001B[0m, in \u001B[0;36m_validate_or_indexify_columns\u001B[1;34m(content, columns)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_mi_list \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(columns) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(content):  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;66;03m# caller's responsibility to check for this...\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    987\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(columns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns passed, passed data had \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(content)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    989\u001B[0m     )\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_mi_list:\n\u001B[0;32m    991\u001B[0m     \u001B[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: 3 columns passed, passed data had 4 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m gn_partitions \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mcommunity\u001B[38;5;241m.\u001B[39mgirvan_newman(G)\n\u001B[0;32m      2\u001B[0m t, s_hops, s_coords, m \u001B[38;5;241m=\u001B[39m single_iter_benchmark(G, gn_partitions, time_threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.75\u001B[39m)\n\u001B[1;32m----> 4\u001B[0m gn_bench \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms_hops\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtime\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msilhouette\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodularity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m gn_bench\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data/benchmarks/girvan-newman.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:851\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    850\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m--> 851\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m \u001B[43mnested_data_to_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;49;00m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;49;00m\n\u001B[0;32m    854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m    857\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    859\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m arrays_to_mgr(\n\u001B[0;32m    860\u001B[0m         arrays,\n\u001B[0;32m    861\u001B[0m         columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    864\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[0;32m    865\u001B[0m     )\n\u001B[0;32m    866\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001B[0m, in \u001B[0;36mnested_data_to_arrays\u001B[1;34m(data, columns, index, dtype)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_named_tuple(data[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    518\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fields)\n\u001B[1;32m--> 520\u001B[0m arrays, columns \u001B[38;5;241m=\u001B[39m \u001B[43mto_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    521\u001B[0m columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001B[0m, in \u001B[0;36mto_arrays\u001B[1;34m(data, columns, dtype)\u001B[0m\n\u001B[0;32m    842\u001B[0m     data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[0;32m    843\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _list_to_arrays(data)\n\u001B[1;32m--> 845\u001B[0m content, columns \u001B[38;5;241m=\u001B[39m \u001B[43m_finalize_columns_and_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m content, columns\n",
      "File \u001B[1;32m~\\switchdrive2\\Institution\\_DIGITAL_NEUROSCIENCE\\Social Media Analytics\\sbb-analytics\\.venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m _validate_or_indexify_columns(contents, columns)\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n\u001B[1;32m--> 942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    944\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(contents) \u001B[38;5;129;01mand\u001B[39;00m contents[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mobject_:\n\u001B[0;32m    945\u001B[0m     contents \u001B[38;5;241m=\u001B[39m convert_object_array(contents, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[1;31mValueError\u001B[0m: 3 columns passed, passed data had 4 columns"
     ]
    }
   ],
   "source": [
    "gn_partitions = nx.community.girvan_newman(G)\n",
    "t, s_hops, s_coords, m = single_iter_benchmark(G, gn_partitions, time_threshold=0.75)\n",
    "\n",
    "gn_bench = pd.DataFrame(zip(t, s_hops, s_coords, m), columns=[\"time\", \"silhouette hops\", \"silhouette coordinates\", \"modularity\"])\n",
    "gn_bench.to_csv(\"./data/benchmarks/girvan-newman.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabee719-3f87-40ea-b359-3ce76f7a6026",
   "metadata": {},
   "source": [
    "## 2.4 Leiden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b2e85d-5f50-4cde-9346-6d29da81d9af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:51:33.587925Z",
     "start_time": "2024-05-21T16:51:33.426313Z"
    }
   },
   "outputs": [],
   "source": [
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "def convert_leiden(G, partition):\n",
    "    idx2name = {idx: name for idx, name in enumerate(list(G.nodes))}\n",
    "    \n",
    "    communities = []\n",
    "    for comm in partition:\n",
    "        communities.append({idx2name[n] for n in comm})\n",
    "        \n",
    "    return communities\n",
    "\n",
    "\n",
    "def leiden_gen(G):\n",
    "    graph = ig.Graph.from_networkx(G, vertex_attr_hashable='name')\n",
    "\n",
    "    # create the partition with every node as its community\n",
    "    partition = la.ModularityVertexPartition(graph)\n",
    "    optimiser = la.Optimiser()\n",
    "        \n",
    "    diff = 1\n",
    "    while diff > 0:\n",
    "        diff = optimiser.optimise_partition(partition)\n",
    "        yield partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7ab41e-562f-4ff2-9f1f-fb1575e85199",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:52:15.071297Z",
     "start_time": "2024-05-21T16:51:36.557766Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       time  silhouette hops  silhouette coordinates  modularity\n0  0.111491         0.149870               -0.010167    0.908269\n1  0.016735         0.148954                0.002682    0.909099\n2  0.018227         0.148926                0.006339    0.909476\n3  0.020094         0.149374                0.009630    0.909565\n4  0.020508         0.148370                0.013778    0.909714\n5  0.017632         0.147884                0.020522    0.909911\n6  0.018069         0.149625                0.025750    0.909934\n7  0.013462         0.144547                0.043134    0.909866\n8  0.018253         0.144547                0.043134    0.909866",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>silhouette hops</th>\n      <th>silhouette coordinates</th>\n      <th>modularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.111491</td>\n      <td>0.149870</td>\n      <td>-0.010167</td>\n      <td>0.908269</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.016735</td>\n      <td>0.148954</td>\n      <td>0.002682</td>\n      <td>0.909099</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.018227</td>\n      <td>0.148926</td>\n      <td>0.006339</td>\n      <td>0.909476</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.020094</td>\n      <td>0.149374</td>\n      <td>0.009630</td>\n      <td>0.909565</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.020508</td>\n      <td>0.148370</td>\n      <td>0.013778</td>\n      <td>0.909714</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.017632</td>\n      <td>0.147884</td>\n      <td>0.020522</td>\n      <td>0.909911</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.018069</td>\n      <td>0.149625</td>\n      <td>0.025750</td>\n      <td>0.909934</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.013462</td>\n      <td>0.144547</td>\n      <td>0.043134</td>\n      <td>0.909866</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.018253</td>\n      <td>0.144547</td>\n      <td>0.043134</td>\n      <td>0.909866</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, s_hops, s_coords, m = multi_iter_benchmark(G, leiden_gen, leiden=True)\n",
    "\n",
    "leiden_bench = pd.DataFrame(zip(t, s_hops, s_coords, m), columns=[\"time\", \"silhouette hops\", \"silhouette coordinates\", \"modularity\"])\n",
    "leiden_bench.to_csv(\"./data/benchmarks/leiden.csv\", index=False)\n",
    "leiden_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8553623-cfe7-4b1f-9aa2-dea43abc7be9",
   "metadata": {},
   "source": [
    "# 3. Evaluation of the Community Detection Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50cb8e6-2d1e-4cee-b300-2cbf3ea9bcec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T16:58:35.449917Z",
     "start_time": "2024-05-21T16:58:35.429721Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bench_results(df_list, labels):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=len(df_list[0].columns), figsize=(18, 6), layout=\"tight\")\n",
    "    # iterate over the columns\n",
    "    for i, col in enumerate(df_list[0].columns):\n",
    "        # iterate over the dataframe: all in the same plot\n",
    "        for df, label in zip(df_list, labels):\n",
    "            axs[i].plot(df[col], label=label)\n",
    "        \n",
    "        axs[i].set_xlabel('Iteration')\n",
    "        axs[i].set_ylabel(col)\n",
    "        axs[i].set_title(f'{col} v. Iteration')\n",
    "            # force int on x-axis\n",
    "        axs[i].xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.savefig(\"community-detection-benchmarks.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe1e05-2299-4132-889b-def8a0b37ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T17:59:10.866437Z",
     "start_time": "2024-05-21T17:59:10.854783Z"
    }
   },
   "outputs": [],
   "source": [
    "louvain_bench = pd.read_csv(\"data/benchmarks/louvain.csv\")\n",
    "louvain_imp_bench = pd.read_csv(\"data/benchmarks/louvain_imp.csv\")\n",
    "gn_bench = pd.read_csv(\"data/benchmarks/girvan-newman.csv\")\n",
    "leiden_bench = pd.read_csv(\"data/benchmarks/leiden.csv\")\n",
    "\n",
    "algorithm_data = [louvain_bench, louvain_imp_bench, leiden_bench, gn_bench[:9]]\n",
    "algorithm_name = [\"Louvain\", \"Louvain Implementation\", \"Leiden\", \"Girvan-Newman\"]\n",
    "\n",
    "# algorithm_data = [louvain_bench, louvain_imp_bench, leiden_bench]\n",
    "# algorithm_name = [\"Louvain\", \"Louvain Implementation\", \"Leiden\"]\n",
    "\n",
    "plot_bench_results(algorithm_data, algorithm_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c746a48-67fc-4f3f-ac6b-307e54685359",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we should be able to safely remove this\n",
    "# kept just in case\n",
    "def eval_plots(*datasets, tick_int):\n",
    "    for i, (data, label, ylabel) in enumerate(datasets):\n",
    "        for d, l in zip(data, label): \n",
    "            ax[i].plot(d, label=l)\n",
    "        ax[i].set_xlabel('Iterations')\n",
    "        ax[i].set_ylabel(ylabel)\n",
    "        ax[i].set_title(f'{ylabel} vs Iterations')\n",
    "        ax[i].xaxis.set_major_locator(ticker.MultipleLocator(tick_int))\n",
    "        ax[i].legend()\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# Plotting with variable names as legend labels\n",
    "#eval_plots(((nx_lou_m_times, nx_gn_m_times[:nx_lou_times.shape[1]]), ('NetworkX Louvain', 'NetworkX Girvan Newman'), 'Time (s)'),\n",
    "            #((nx_lou_m_sil, nx_gn_m_sil[:nx_lou_times.shape[1]]), ('NetworkX Louvain', 'NetworkX Girvan Newman'), 'Silhouette Score'),\n",
    "            #((nx_lou_m_mod, nx_gn_m_mod[:nx_lou_times.shape[1]]), ('NetworkX Louvain', 'NetworkX Girvan Newman'), 'Modularity Score'), \n",
    "           #tick_int=1)\n",
    "\n",
    "\n",
    "# Just Girvan Newman plots\n",
    "# read nx gn csvs\n",
    "nx_gn_times = pd.read_csv(\"data/communities/nx_gn_iteration_times.csv\")\n",
    "nx_gn_m_times = nx_gn_times.mean(axis='rows')\n",
    "\n",
    "nx_gn_sil = pd.read_csv(\"data/communities/nx_gn_silhouette_scores_per_iteration.csv\")\n",
    "nx_gn_m_sil = nx_gn_sil.mean(axis='rows')\n",
    "\n",
    "nx_gn_mod = pd.read_csv(\"data/communities/nx_gn_modularity_scores_per_iteration.csv\")\n",
    "nx_gn_m_mod = nx_gn_mod.mean(axis='rows')\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "eval_plots(((nx_gn_m_times,), ('NetworkX Girvan Newman',), 'Time (s)'),\n",
    "            ((nx_gn_m_sil,), ('NetworkX Girvan Newman',), 'Silhouette Score'),\n",
    "            ((nx_gn_m_mod,), ('NetworkX Girvan Newman',), 'Modularity Score'),\n",
    "           tick_int=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
